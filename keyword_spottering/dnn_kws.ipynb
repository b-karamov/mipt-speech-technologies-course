{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Детекция фраз \"не слышу\" / \"не слышно\"\n",
    "\n",
    "Бинарная классификация звонков: метка 1, если в аудио есть хотя бы одна из целевых фраз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio.functional as AF\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../mel_spectrogram\")\n",
    "from mel_spectrogram import compute_mel_spectrogram, load_audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "print(\"device =\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Хелперы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_deltas(m: np.ndarray) -> np.ndarray:\n",
    "    d = np.zeros_like(m)\n",
    "    d[:, 1:] = m[:, 1:] - m[:, :-1]\n",
    "    return d\n",
    "\n",
    "\n",
    "def _resample(wav: np.ndarray, sr: int, target_sr: int) -> tuple[np.ndarray, int]:\n",
    "    \"\"\"Поменять sample_rate.\"\"\"\n",
    "    if sr == target_sr:\n",
    "        return wav, sr\n",
    "    tensor = torch.tensor(wav, dtype=torch.float32)\n",
    "    resampled = AF.resample(tensor, sr, target_sr)\n",
    "    return resampled.numpy(), target_sr\n",
    "\n",
    "\n",
    "def compute_mel_cached(\n",
    "    path: Path,\n",
    "    cache_dir: Path | None,\n",
    "    *,\n",
    "    target_sr: int,\n",
    "    n_mels: int,\n",
    "    window_ms: float,\n",
    "    hop_ms: float,\n",
    "    f_min: float,\n",
    "    f_max: float | None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Рассчитать mel-спектрограмму. Если есть закэшированная, взять ее, иначе закэшировать.\"\"\"\n",
    "    cache_path = None\n",
    "    if cache_dir is not None:\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        cache_path = cache_dir / f\"{path.stem}_sr{target_sr}_m{n_mels}_w{int(window_ms)}_h{int(hop_ms)}.npz\"\n",
    "        if cache_path.exists():\n",
    "            return np.load(cache_path)[\"mel\"]\n",
    "\n",
    "    wav, sr = load_audio(str(path), mono=True)\n",
    "    wav, sr = _resample(wav, sr, target_sr)\n",
    "    mel = compute_mel_spectrogram(\n",
    "        wav,\n",
    "        sr,\n",
    "        window_ms=window_ms,\n",
    "        hop_ms=hop_ms,\n",
    "        n_mels=n_mels,\n",
    "        f_min=f_min,\n",
    "        f_max=f_max,\n",
    "        norm=\"slaney\",\n",
    "        log_scale=True,\n",
    "        eps=1e-12,\n",
    "    ).mel_spectrogram\n",
    "\n",
    "    if cache_path is not None:\n",
    "        np.savez(cache_path, mel=mel)\n",
    "    return mel\n",
    "\n",
    "\n",
    "def window_mel(mel: np.ndarray, start_frame: int, target_frames: int) -> np.ndarray:\n",
    "    \"\"\"Вырезать окно начиная с позиции start_frame.\"\"\"\n",
    "    pad_value = float(mel.min())\n",
    "    end_frame = start_frame + target_frames\n",
    "    if end_frame > mel.shape[1]:\n",
    "        mel = np.pad(\n",
    "            mel,\n",
    "            ((0, 0), (0, end_frame - mel.shape[1])),\n",
    "            mode=\"constant\",\n",
    "            constant_values=pad_value,\n",
    "        )\n",
    "    return mel[:, start_frame:end_frame]\n",
    "\n",
    "\n",
    "def _pool_time(x: np.ndarray, out_T: int) -> np.ndarray:\n",
    "    \"\"\"Усреднить по времени до out_T бинов.\"\"\"\n",
    "    T = x.shape[-1]\n",
    "    if T == out_T:\n",
    "        return x\n",
    "    edges = np.linspace(0, T, out_T + 1).astype(int)\n",
    "    pooled = []\n",
    "    for i in range(out_T):\n",
    "        a, b = int(edges[i]), int(edges[i + 1])\n",
    "        pooled.append(x[..., a:b].mean(axis=-1, keepdims=True))\n",
    "    return np.concatenate(pooled, axis=-1)\n",
    "\n",
    "\n",
    "def features_from_mel(mel: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"Фичи для DNN.\"\"\"\n",
    "    d1 = _compute_deltas(mel)\n",
    "    d2 = _compute_deltas(d1)\n",
    "    x = np.stack([mel, d1, d2], axis=0)  # (3, n_mels, T)\n",
    "    x = _pool_time(x, out_T=50)          # (3, n_mels, 50)\n",
    "    mu = x.mean(axis=-1, keepdims=True)\n",
    "    sigma = x.std(axis=-1, keepdims=True) + 1e-5\n",
    "    x = (x - mu) / sigma\n",
    "    return torch.from_numpy(x.reshape(-1).astype(np.float32))\n",
    "\n",
    "\n",
    "def collect_audio_files(audio_dir: Path) -> list[Path]:\n",
    "    files = sorted(audio_dir.glob(\"*.opus\"))\n",
    "    return files\n",
    "\n",
    "\n",
    "def load_bounds(path: Path) -> dict[str, tuple[float, float]]:\n",
    "    \"\"\"Загрузить конфиг с границами таргетов.\"\"\"\n",
    "    raw = json.loads(path.read_text())\n",
    "    return {k: (float(v[0]), float(v[1])) for k, v in raw.items()}\n",
    "\n",
    "\n",
    "def train_val_split(files: list[Path], val_ratio: float = 0.15, seed: int = 42):\n",
    "    \"\"\"Разбить на train/val.\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    files = list(files)\n",
    "    rng.shuffle(files)\n",
    "    n_val = max(1, int(len(files) * val_ratio))\n",
    "    return files[n_val:], files[:n_val]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallQualityDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        files: list[Path],\n",
    "        bounds: dict[str, tuple[float, float]],\n",
    "        segment_seconds: float = 2.5,\n",
    "        n_segments_per_file: int = 4,\n",
    "        target_sr: int = 16000,\n",
    "        n_mels: int = 48,\n",
    "        window_ms: float = 25.0,\n",
    "        hop_ms: float = 10.0,\n",
    "        cache_dir: Path | None = None,\n",
    "    ):\n",
    "        self.files = files\n",
    "        self.bounds = bounds\n",
    "        self.segment_seconds = segment_seconds\n",
    "        self.n_segments_per_file = n_segments_per_file\n",
    "        self.target_sr = target_sr\n",
    "        self.n_mels = n_mels\n",
    "        self.window_ms = window_ms\n",
    "        self.hop_ms = hop_ms\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        self.frames_per_sec = 1000.0 / hop_ms\n",
    "        self.target_frames = int(round(segment_seconds * self.frames_per_sec))\n",
    "        self.feature_dim = 3 * n_mels * 50\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def _start_time(self, total_sec: float, label: int, bounds: tuple[float, float] | None) -> float:\n",
    "        if label == 0 or bounds is None:\n",
    "            # Если нет таргета, взять случайное начало\n",
    "            return random.uniform(0.0, max(total_sec - self.segment_seconds, 0.0))\n",
    "        start, end = bounds\n",
    "        center = 0.5 * (start + end)\n",
    "        st = max(0.0, center - self.segment_seconds / 2)\n",
    "        return min(st, max(total_sec - self.segment_seconds, 0.0))\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        path = self.files[idx]\n",
    "        uid = path.stem\n",
    "        label = 1 if uid in self.bounds else 0\n",
    "        bounds = self.bounds.get(uid)\n",
    "\n",
    "        mel = compute_mel_cached(\n",
    "            path,\n",
    "            cache_dir=self.cache_dir,\n",
    "            target_sr=self.target_sr,\n",
    "            n_mels=self.n_mels,\n",
    "            window_ms=self.window_ms,\n",
    "            hop_ms=self.hop_ms,\n",
    "            f_min=50.0,\n",
    "            f_max=7800.0,\n",
    "        )\n",
    "\n",
    "        total_sec = mel.shape[1] / self.frames_per_sec\n",
    "\n",
    "        starts = []\n",
    "        if label == 1 and bounds is not None:\n",
    "            starts.append(self._start_time(total_sec, label, bounds))\n",
    "            for _ in range(max(self.n_segments_per_file - 1, 0)):\n",
    "                starts.append(random.uniform(0.0, max(total_sec - self.segment_seconds, 0.0)))\n",
    "        else:\n",
    "            for _ in range(max(self.n_segments_per_file, 1)):\n",
    "                starts.append(random.uniform(0.0, max(total_sec - self.segment_seconds, 0.0)))\n",
    "\n",
    "        feats = []\n",
    "        for start_sec in starts:\n",
    "            start_frame = int(start_sec * self.frames_per_sec)\n",
    "            window = window_mel(mel, start_frame, self.target_frames)\n",
    "            feats.append(features_from_mel(window))\n",
    "        x = torch.stack(feats, dim=0)  # (n_segments, feature_dim)\n",
    "        y = torch.tensor(label, dtype=torch.float32)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNKWS(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden=(256, 128), dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        # Нормализация\n",
    "        layers = [nn.LayerNorm(input_dim)]\n",
    "        d = input_dim\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(d, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            d = h\n",
    "        layers += [nn.Linear(d, 1)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение и инференс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pos_weight(files: list[Path], bounds: dict[str, tuple[float, float]]):\n",
    "    pos = sum(1 for f in files if f.stem in bounds)\n",
    "    neg = max(len(files) - pos, 1)\n",
    "    pos = max(pos, 1)\n",
    "    return torch.tensor([neg / pos], dtype=torch.float32)\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, opt, pos_weight=None):\n",
    "    model.train()\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    total_loss = 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        logits = model(x)\n",
    "        # x - (B, K, D), так как K окон на файл, делаем max-pool по окнам.\n",
    "        logits = logits.max(dim=1).values\n",
    "        loss = criterion(logits, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_file(\n",
    "    model,\n",
    "    path: Path,\n",
    "    segment_seconds: float = 2.5,\n",
    "    hop_seconds: float = 1.0,\n",
    "    target_sr: int = 16000,\n",
    "    n_mels: int = 48,\n",
    "    window_ms: float = 25.0,\n",
    "    hop_ms: float = 10.0,\n",
    "    cache_dir: Path | None = None,\n",
    ") -> float:\n",
    "    mel = compute_mel_cached(\n",
    "        path,\n",
    "        cache_dir=cache_dir,\n",
    "        target_sr=target_sr,\n",
    "        n_mels=n_mels,\n",
    "        window_ms=window_ms,\n",
    "        hop_ms=hop_ms,\n",
    "        f_min=50.0,\n",
    "        f_max=7800.0,\n",
    "    )\n",
    "    frames_per_sec = 1000.0 / hop_ms\n",
    "    target_frames = int(round(segment_seconds * frames_per_sec))\n",
    "    hop_frames = max(1, int(round(hop_seconds * frames_per_sec)))\n",
    "\n",
    "    probs = []\n",
    "    start = 0\n",
    "    while start < mel.shape[1]:\n",
    "        window = window_mel(mel, start, target_frames)\n",
    "        feat = features_from_mel(window).to(DEVICE)\n",
    "        prob = torch.sigmoid(model(feat.unsqueeze(0))).item()\n",
    "        probs.append(prob)\n",
    "        if start + hop_frames >= mel.shape[1] and start != 0:\n",
    "            break\n",
    "        start += hop_frames\n",
    "    return max(probs) if probs else 0.0\n",
    "\n",
    "\n",
    "def _metrics_from_counts(tp: int, fp: int, fn: int, tn: int):\n",
    "    pos = tp + fn\n",
    "    neg = tn + fp\n",
    "    far = fp / max(neg, 1)\n",
    "    frr = fn / max(pos, 1)\n",
    "    score = 0.0\n",
    "    if (1 - frr + 1 - far) > 0:\n",
    "        score = 2 * (1 - frr) * (1 - far) / (1 - frr + 1 - far)\n",
    "    return score, far, frr\n",
    "\n",
    "\n",
    "def evaluate_on_files(\n",
    "    model,\n",
    "    files: list[Path],\n",
    "    bounds: dict[str, tuple[float, float]],\n",
    "    threshold: float = 0.5,\n",
    "    thresholds: list[float] | None = None,\n",
    "    segment_seconds: float = 2.5,\n",
    "    hop_seconds: float = 1.0,\n",
    "    target_sr: int = 16000,\n",
    "    n_mels: int = 48,\n",
    "    window_ms: float = 25.0,\n",
    "    hop_ms: float = 10.0,\n",
    "    cache_dir: Path | None = None,\n",
    "):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    y_true_list = []\n",
    "    prob_list = []\n",
    "    for path in files:\n",
    "        y_true = 1 if path.stem in bounds else 0\n",
    "        prob = predict_file(\n",
    "            model,\n",
    "            path,\n",
    "            segment_seconds=segment_seconds,\n",
    "            hop_seconds=hop_seconds,\n",
    "            target_sr=target_sr,\n",
    "            n_mels=n_mels,\n",
    "            window_ms=window_ms,\n",
    "            hop_ms=hop_ms,\n",
    "            cache_dir=cache_dir,\n",
    "        )\n",
    "        y_true_list.append(y_true)\n",
    "        prob_list.append(prob)\n",
    "\n",
    "    y_true_arr = np.asarray(y_true_list, dtype=np.int8)\n",
    "    prob_arr = np.asarray(prob_list, dtype=np.float32)\n",
    "\n",
    "    cand_thresholds = thresholds or [threshold]\n",
    "    best = None\n",
    "    for th in cand_thresholds:\n",
    "        y_pred = prob_arr >= float(th)\n",
    "        tp = int(((y_true_arr == 1) & y_pred).sum())\n",
    "        fp = int(((y_true_arr == 0) & y_pred).sum())\n",
    "        fn = int(((y_true_arr == 1) & (~y_pred)).sum())\n",
    "        tn = int(((y_true_arr == 0) & (~y_pred)).sum())\n",
    "        score, far, frr = _metrics_from_counts(tp, fp, fn, tn)\n",
    "        if best is None or score > best[\"score\"]:\n",
    "            best = {\n",
    "                \"threshold\": float(th),\n",
    "                \"score\": float(score),\n",
    "                \"far\": float(far),\n",
    "                \"frr\": float(frr),\n",
    "                \"tp\": tp,\n",
    "                \"fp\": fp,\n",
    "                \"fn\": fn,\n",
    "                \"tn\": tn,\n",
    "            }\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "def make_submission(\n",
    "    model,\n",
    "    files: list[Path],\n",
    "    out_path: Path,\n",
    "    threshold: float = 0.5,\n",
    "    segment_seconds: float = 2.5,\n",
    "    hop_seconds: float = 1.0,\n",
    "    target_sr: int = 16000,\n",
    "    n_mels: int = 48,\n",
    "    window_ms: float = 25.0,\n",
    "    hop_ms: float = 10.0,\n",
    "    cache_dir: Path | None = None,\n",
    "):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    rows = [\"id,label\"]\n",
    "    for path in files:\n",
    "        prob = predict_file(\n",
    "            model,\n",
    "            path,\n",
    "            segment_seconds=segment_seconds,\n",
    "            hop_seconds=hop_seconds,\n",
    "            target_sr=target_sr,\n",
    "            n_mels=n_mels,\n",
    "            window_ms=window_ms,\n",
    "            hop_ms=hop_ms,\n",
    "            cache_dir=cache_dir,\n",
    "        )\n",
    "        label = 1 if prob >= threshold else 0\n",
    "        rows.append(f\"{path.stem},{label}\")\n",
    "    out_path.write_text(\"\\n\".join(rows))\n",
    "    if was_training:\n",
    "        model.train()\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [03:12<45:01, 192.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=01 loss=0.5007 score=0.5829 far=0.3508 frr=0.4710 thr=0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [06:27<42:02, 194.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=02 loss=0.4143 score=0.6277 far=0.3508 frr=0.3925 thr=0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [09:39<38:37, 193.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=03 loss=0.3768 score=0.6453 far=0.3637 frr=0.3455 thr=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [12:50<35:14, 192.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=04 loss=0.3452 score=0.6585 far=0.2793 frr=0.3938 thr=0.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [15:57<31:43, 190.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=05 loss=0.3270 score=0.6636 far=0.3175 frr=0.3542 thr=0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [19:03<28:19, 188.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=06 loss=0.3117 score=0.6764 far=0.3281 frr=0.3191 thr=0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [22:10<25:04, 188.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=07 loss=0.2995 score=0.6733 far=0.2579 frr=0.3838 thr=0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [25:20<22:00, 188.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=08 loss=0.2869 score=0.6874 far=0.2820 frr=0.3406 thr=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [28:31<18:57, 189.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=09 loss=0.2801 score=0.6818 far=0.3058 frr=0.3301 thr=0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [31:44<15:52, 190.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10 loss=0.2730 score=0.6828 far=0.2973 frr=0.3360 thr=0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [34:56<12:43, 190.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=11 loss=0.2615 score=0.6870 far=0.2716 frr=0.3499 thr=0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [38:06<09:32, 190.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=12 loss=0.2569 score=0.6910 far=0.2649 frr=0.3482 thr=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [41:15<06:20, 190.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=13 loss=0.2511 score=0.6936 far=0.2756 frr=0.3347 thr=0.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [44:26<03:10, 190.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=14 loss=0.2439 score=0.6938 far=0.2813 frr=0.3294 thr=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=15 loss=0.2374 score=0.6901 far=0.2652 frr=0.3495 thr=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "TRAIN_ROOT = Path(\"./train_opus\")\n",
    "TEST_ROOT = Path(\"./test_opus\")\n",
    "TRAIN_AUDIO = TRAIN_ROOT / \"audio\"\n",
    "TEST_AUDIO = TEST_ROOT / \"audio\"\n",
    "BOUNDS_PATH = TRAIN_ROOT / \"word_bounds.json\"\n",
    "CACHE_DIR = Path(\"./mel_cache\")\n",
    "\n",
    "segment_seconds = 2.5\n",
    "hop_seconds = 1.0\n",
    "n_mels = 48\n",
    "\n",
    "bounds = load_bounds(BOUNDS_PATH)\n",
    "all_files = collect_audio_files(TRAIN_AUDIO)\n",
    "train_files, val_files = train_val_split(all_files, val_ratio=0.15, seed=SEED)\n",
    "\n",
    "train_ds = CallQualityDataset(\n",
    "    train_files,\n",
    "    bounds=bounds,\n",
    "    segment_seconds=segment_seconds,\n",
    "    n_segments_per_file=4,\n",
    "    target_sr=16000,\n",
    "    n_mels=n_mels,\n",
    "    cache_dir=CACHE_DIR,\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "model = DNNKWS(input_dim=train_ds.feature_dim, hidden=(512, 256), dropout=0.1).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "pos_weight = compute_pos_weight(train_files, bounds).to(DEVICE)\n",
    "\n",
    "best_threshold = 0.5\n",
    "threshold_grid = np.linspace(0.05, 0.95, 19).tolist()\n",
    "\n",
    "for epoch in tqdm(range(1, 16), leave=False):\n",
    "    loss = train_epoch(model, train_loader, opt, pos_weight=pos_weight)\n",
    "    metrics = evaluate_on_files(\n",
    "        model,\n",
    "        val_files,\n",
    "        bounds,\n",
    "        thresholds=threshold_grid,\n",
    "        segment_seconds=segment_seconds,\n",
    "        hop_seconds=hop_seconds,\n",
    "        target_sr=16000,\n",
    "        n_mels=n_mels,\n",
    "        cache_dir=CACHE_DIR,\n",
    "    )\n",
    "    best_threshold = metrics[\"threshold\"]\n",
    "    print(\n",
    "        f\"epoch={epoch:02d} loss={loss:.4f} score={metrics['score']:.4f} \"\n",
    "        f\"far={metrics['far']:.4f} frr={metrics['frr']:.4f} thr={best_threshold:.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNNKWS(\n",
       "  (net): Sequential(\n",
       "    (0): LayerNorm((7200,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=7200, out_features=512, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Dropout(p=0.1, inplace=False)\n",
       "    (7): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сабмит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохранено в submition.csv с 27000 записями\n"
     ]
    }
   ],
   "source": [
    "test_files = collect_audio_files(TEST_AUDIO)\n",
    "out_csv = make_submission(\n",
    "    model,\n",
    "    test_files,\n",
    "    out_path=Path(\"submition.csv\"),\n",
    "    threshold=best_threshold,\n",
    "    segment_seconds=segment_seconds,\n",
    "    hop_seconds=hop_seconds,\n",
    "    target_sr=16000,\n",
    "    n_mels=n_mels,\n",
    "    cache_dir=CACHE_DIR,\n",
    ")\n",
    "print(f\"Сохранено в {out_csv} с {len(test_files)} записями\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
